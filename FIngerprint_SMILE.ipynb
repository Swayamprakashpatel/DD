{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLpyEAqCK24WxOIPky4iAH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swayamprakashpatel/DD/blob/main/FIngerprint_SMILE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "48UvgJj1tjT_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DOWNLOAD DATA FROM KAGGLE\n",
        "# DOWNLOAD DATA FROM KAGGLE (!IMPORTANT!: REFRESH RUNTIME BEFORE RE-RUNNING THE CODE)\n",
        "#%%capture\n",
        "from google.colab import files\n",
        "files.upload()  #this will prompt you to upload the kaggle.json\n",
        "\n",
        "#Make Directory of Kaggle and set its permission for access.\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json  # set permission\n",
        "\n",
        "# Download Data from Kaggle Fast and Unzip them in /content\n",
        "!kaggle datasets download -d drswayamprakashpatel/DD-Dataset-csv  -p /content # For model download\n",
        "\n",
        "#Unzip data (Two Folders - Training and Validation)\n",
        "import os\n",
        "os.chdir('/content')\n",
        "#create a directory named train/\n",
        "!unzip -q DD-Dataset-csv.zip #Unzip Model"
      ],
      "metadata": {
        "id": "KUhifzm9t995"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/DATASET_Final.csv')  # Replace with the path to your dataset\n",
        "#data = data.iloc[0:10000,:]\n",
        "# Split the data into input and output columns\n",
        "X = data.iloc[:, 7:888]  # Input columns\n",
        "y = data['SMILE']      # Output column\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Reshape the input data\n",
        "X_train_reshaped = X_train.values.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_test_reshaped = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "# Preprocess the output data (SMILES)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(y_train)\n",
        "y_train_sequences = tokenizer.texts_to_sequences(y_train)\n",
        "y_test_sequences = tokenizer.texts_to_sequences(y_test)\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_sequence_length = max(len(seq) for seq in y_train_sequences + y_test_sequences)\n",
        "\n",
        "# Pad sequences to ensure consistent length\n",
        "y_train_padded = pad_sequences(y_train_sequences, maxlen=max_sequence_length)\n",
        "y_test_padded = pad_sequences(y_test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Define the model\n",
        "input_shape = (1, X_train.shape[1])\n",
        "output_vocab_size = len(tokenizer.word_index) + 1\n",
        "hidden_units = 28\n",
        "\n",
        "inputs = Input(shape=input_shape)\n",
        "\n",
        "##ENCODER###\n",
        "encoder = LSTM(hidden_units, return_sequences=True)(inputs) #return_sequences=True in the first LSTM layer (encoder). This will ensure that the output of the first LSTM layer is a sequence, which can then be passed as input to the second LSTM layer.\n",
        "encoder_1 = LSTM(hidden_units, return_sequences=False)(encoder) # Extra LSTM Layer: \n",
        "hidden_layer = Dense(hidden_units, activation='relu')(encoder_1)# Hidden layer in encoder\n",
        "\n",
        "##DECODER##\n",
        "decoder = Dense(hidden_units, activation='relu')(hidden_layer)\n",
        "hidden_layer_1 = Dense(hidden_units, activation='relu')(decoder) #Hidden laye rin decoder\n",
        "outputs = Dense(max_sequence_length, activation='softmax')(hidden_layer_1)  # Updated output layer units\n",
        "\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "#Train Model\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "filepath = '/content/drive/MyDrive/Model_DE/PF_SML_Model.hdf5'\n",
        "\n",
        "monitor = 'val_accuracy'\n",
        "mode = 'max'\n",
        " \n",
        "checkpoint = [tf.keras.callbacks.ModelCheckpoint(filepath, monitor = monitor, mode = mode, save_best_only=True, Save_weights_only = False, verbose = 1), \n",
        "              tf.keras.callbacks.EarlyStopping(monitor= monitor, mode = mode, patience=250, verbose =1), [tensorboard_callback]]\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate = 0.1), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "hist = model.fit(X_train_reshaped, y_train_padded, epochs= 2000, callbacks=[checkpoint],validation_data=(X_test_reshaped, y_test_padded), batch_size= None)"
      ],
      "metadata": {
        "id": "bSCl7tzeFv0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train_padded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjV8bNLMI4cf",
        "outputId": "ea453c94-729f-40c6-8bc7-0205c32fe301"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21863, 324)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#FOR PREDICTION\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/Model_DE/PF_SML_Model.hdf5')\n",
        "\n",
        "# Prepare the input data\n",
        "input_data = X_test.iloc[0].values  # Replace with your input data of length 881\n",
        "input_reshaped = input_data.reshape(1, 1, len(input_data))\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(input_reshaped)\n",
        "\n",
        "# Decode the predicted sequence\n",
        "predicted_sequence = tokenizer.sequences_to_texts(predictions.argmax(axis=2))[0]"
      ],
      "metadata": {
        "id": "buQAWrxfZVC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# different approach\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/DATASET_Final.csv')  # Replace with the path to your dataset\n",
        "data = data.iloc[0:10,:]\n",
        "# Split the data into input and output columns\n",
        "X = data.iloc[:, 7:888]  # Input columns\n",
        "y = data['SMILE']  # Output column\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# Preprocess the output data (SMILES)\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(y_train)\n",
        "y_train_sequences = tokenizer.texts_to_sequences(y_train)\n",
        "y_test_sequences = tokenizer.texts_to_sequences(y_test)\n",
        "\n",
        "# Find the maximum sequence length\n",
        "max_sequence_length = max(len(seq) for seq in y_train_sequences + y_test_sequences)\n",
        "\n",
        "# Pad sequences to ensure consistent length\n",
        "y_train_padded = pad_sequences(y_train_sequences, maxlen=max_sequence_length)\n",
        "y_test_padded = pad_sequences(y_test_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "\n",
        "\n",
        "# Define the model\n",
        "input_shape = (X_train.shape[1],)\n",
        "output_classes = y_train_padded.shape[1]\n",
        "\n",
        "inputs = Input(shape=input_shape)\n",
        "hidden_layer = Dense(64, activation='relu')(inputs)\n",
        "outputs = Dense(output_classes, activation='sigmoid')(hidden_layer)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "filepath = '/content/drive/MyDrive/Model_DE/PF_SML_Model.hdf5'\n",
        "\n",
        "monitor = 'val_accuracy'\n",
        "mode = 'max'\n",
        " \n",
        "checkpoint = [tf.keras.callbacks.ModelCheckpoint(filepath, monitor = monitor, mode = mode, save_best_only=True, Save_weights_only = False, verbose = 1), \n",
        "              tf.keras.callbacks.EarlyStopping(monitor= monitor, mode = mode, patience=250, verbose =1), [tensorboard_callback]]\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = 0.1), loss=tf.keras.losses.CategoricalCrossentropy(from_logits = False), metrics=['accuracy'])\n",
        "hist = model.fit(X_train, y_train_padded, epochs= 2000, callbacks=[checkpoint],validation_data=(X_test, y_test_padded), batch_size= None)\n"
      ],
      "metadata": {
        "id": "Av9xi5VO7cU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_sequences = tokenizer.sequences_to_texts(y_train_padded[0:1,:])\n",
        "\n",
        "# Print the original sequences\n",
        "print(original_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etcNXErM9_I1",
        "outputId": "3cf37be1-7d5c-4cfb-9550-1e046b9fd54a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['c c ( c ) c ( c ( = o ) o ) n s ( = o ) ( = o ) c 1 = c c = c ( c = c 1 ) c 2 = c c = c ( c = c 2 ) b r']\n"
          ]
        }
      ]
    }
  ]
}