{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMqYQLOaomxSl1FMgexoFQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swayamprakashpatel/DD/blob/main/Drug_Discovery_15_2_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGjllWymy5bL",
        "outputId": "3b81ad0a-a375-4dbf-8f3d-2bf3d482f195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('final_output_15_2_25.csv')\n",
        "\n",
        "# Extract necessary columns\n",
        "protein_sequences = data['Sequence'].values\n",
        "smiles_strings = data['SMILE'].values\n",
        "\n",
        "# 1. Preprocess protein sequences (One-hot encoding)\n",
        "def one_hot_encoding(protein_seq):\n",
        "    amino_acids = 'ACDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    aa_dict = {amino_acids[i]: i for i in range(len(amino_acids))}\n",
        "    num_aa = len(aa_dict)\n",
        "\n",
        "    max_seq_len = 5038  # Maximum protein sequence length\n",
        "    one_hot = np.zeros((len(protein_seq), max_seq_len, num_aa))  # Pad all sequences to max length\n",
        "\n",
        "    for i, seq in enumerate(protein_seq):\n",
        "        for j, aa in enumerate(seq):\n",
        "            if aa in aa_dict:\n",
        "                one_hot[i, j, aa_dict[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Apply one-hot encoding to protein sequences\n",
        "X_seq = one_hot_encoding(protein_sequences)\n",
        "\n",
        "# 2. Preprocess SMILES strings (Integer encoding)\n",
        "def smiles_to_int(smiles_strings, max_length=1000):\n",
        "    char_set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789()=#[]+-'\n",
        "    char_dict = {char: i+1 for i, char in enumerate(char_set)}  # Map chars to integers\n",
        "\n",
        "    smiles_int = np.zeros((len(smiles_strings), max_length), dtype=int)\n",
        "\n",
        "    for i, smile in enumerate(smiles_strings):\n",
        "        for j, char in enumerate(smile):\n",
        "            if j < max_length:\n",
        "                smiles_int[i, j] = char_dict.get(char, 0)  # Map characters to integers\n",
        "    return smiles_int\n",
        "\n",
        "# Apply integer encoding to SMILES strings\n",
        "X_smiles = smiles_to_int(smiles_strings)\n",
        "\n",
        "# 3. Split the data into training and test sets\n",
        "X_train_seq, X_test_seq, X_train_smiles, X_test_smiles = train_test_split(\n",
        "    X_seq, X_smiles, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Pad the SMILES sequences (to 1000)\n",
        "X_train_smiles = pad_sequences(X_train_smiles, maxlen=1000, padding='post')\n",
        "X_test_smiles = pad_sequences(X_test_smiles, maxlen=1000, padding='post')\n",
        "\n",
        "# 5. Prepare target variable (assuming y is your target, with 1000 classes)\n",
        "y_train = np.zeros(len(X_train_smiles))  # Dummy binary target variable\n",
        "y_test = np.zeros(len(X_test_smiles))  # Dummy binary target variable\n",
        "\n",
        "# 6. One-hot encode the target variable\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=1000)  # One-hot encoding\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=1000)\n",
        "\n",
        "# 7. Build the model\n",
        "\n",
        "# Protein sequence model (1D Conv)\n",
        "seq_input = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (5038, 25)\n",
        "x = Conv1D(64, 3, activation='relu')(seq_input)\n",
        "x = MaxPooling1D(2)(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "# SMILES sequence model (Dense)\n",
        "smiles_input = Input(shape=(X_train_smiles.shape[1],))  # (1000,)\n",
        "y = Dense(128, activation='relu')(smiles_input)\n",
        "\n",
        "# Merge both models\n",
        "merged = tf.keras.layers.concatenate([x, y])\n",
        "\n",
        "# Output layer for multi-class classification\n",
        "output = Dense(1000, activation='softmax')(merged)  # Output for 1000 classes\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[seq_input, smiles_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# 8. Train the model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_smiles], y_train,\n",
        "    epochs=100,\n",
        "    batch_size=200,\n",
        "    validation_data=([X_test_seq, X_test_smiles], y_test)\n",
        ")\n",
        "\n",
        "# 9. Evaluate the model\n",
        "test_loss, test_acc = model.evaluate([X_test_seq, X_test_smiles], y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# Optionally, plot training history (accuracy and loss)\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "WxzzxufZAa95",
        "outputId": "ae264259-a25c-4976-8b63-1d191aa25005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5038\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5036\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │          \u001b[38;5;34m4,864\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2518\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161152\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m128,128\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161280\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
              "│                           │                        │                │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │    \u001b[38;5;34m161,281,000\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5038</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5036</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2518</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161152</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128,128</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161280</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
              "│                           │                        │                │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">161,281,000</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m161,413,992\u001b[0m (615.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,413,992</span> (615.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m161,413,992\u001b[0m (615.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,413,992</span> (615.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1358s\u001b[0m 11s/step - accuracy: 0.9573 - loss: 0.3992 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\u001b[1m 13/126\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18:44\u001b[0m 10s/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('final_output_15_2_25.csv')\n",
        "\n",
        "# Extract necessary columns\n",
        "protein_sequences = data['Sequence'].values\n",
        "smiles_strings = data['SMILE'].values\n",
        "\n",
        "# 1. Preprocess protein sequences (One-hot encoding)\n",
        "def one_hot_encoding(protein_seq):\n",
        "    amino_acids = 'ACDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    aa_dict = {amino_acids[i]: i for i in range(len(amino_acids))}\n",
        "    num_aa = len(aa_dict)\n",
        "\n",
        "    max_seq_len = 5038  # Maximum protein sequence length\n",
        "    one_hot = np.zeros((len(protein_seq), max_seq_len, num_aa))  # Pad all sequences to max length\n",
        "\n",
        "    for i, seq in enumerate(protein_seq):\n",
        "        for j, aa in enumerate(seq):\n",
        "            if aa in aa_dict:\n",
        "                one_hot[i, j, aa_dict[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Apply one-hot encoding to protein sequences\n",
        "X_seq = one_hot_encoding(protein_sequences)\n",
        "\n",
        "# 2. Preprocess SMILES strings (Integer encoding)\n",
        "def smiles_to_int(smiles_strings, max_length=1000):\n",
        "    char_set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789()=#[]+-'\n",
        "    char_dict = {char: i+1 for i, char in enumerate(char_set)}  # Map chars to integers\n",
        "\n",
        "    smiles_int = np.zeros((len(smiles_strings), max_length), dtype=int)\n",
        "\n",
        "    for i, smile in enumerate(smiles_strings):\n",
        "        for j, char in enumerate(smile):\n",
        "            if j < max_length:\n",
        "                smiles_int[i, j] = char_dict.get(char, 0)  # Map characters to integers\n",
        "    return smiles_int\n",
        "\n",
        "# Apply integer encoding to SMILES strings\n",
        "X_smiles = smiles_to_int(smiles_strings)\n",
        "\n",
        "# 3. Split the data into training and test sets\n",
        "X_train_seq, X_test_seq, X_train_smiles, X_test_smiles = train_test_split(\n",
        "    X_seq, X_smiles, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Pad the SMILES sequences (to 1000)\n",
        "X_train_smiles = pad_sequences(X_train_smiles, maxlen=1000, padding='post')\n",
        "X_test_smiles = pad_sequences(X_test_smiles, maxlen=1000, padding='post')\n",
        "\n",
        "# 5. Prepare target variable (assuming y is your target, with 1000 classes)\n",
        "y_train = np.zeros(len(X_train_smiles))  # Dummy binary target variable\n",
        "y_test = np.zeros(len(X_test_smiles))  # Dummy binary target variable\n",
        "\n",
        "# 6. One-hot encode the target variable\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=1000)  # One-hot encoding\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=1000)\n",
        "\n",
        "# 7. Build the model\n",
        "\n",
        "# Protein sequence model (1D Conv)\n",
        "seq_input = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (5038, 25)\n",
        "x = Conv1D(64, 3, activation='relu')(seq_input)\n",
        "x = MaxPooling1D(2)(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "# SMILES sequence model (Dense)\n",
        "smiles_input = Input(shape=(X_train_smiles.shape[1],))  # (1000,)\n",
        "y = Dense(128, activation='relu')(smiles_input)\n",
        "\n",
        "# Merge both models\n",
        "merged = tf.keras.layers.concatenate([x, y])\n",
        "\n",
        "# Output layer for multi-class classification\n",
        "output = Dense(1000, activation='softmax')(merged)  # Output for 1000 classes\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[seq_input, smiles_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# 8. Set up Model Checkpoint to save the best model during training\n",
        "checkpoint_dir = './model_checkpoints'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'best_model.h5'),  # Save best model as .h5 file\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    save_best_only=True,  # Save the best model only\n",
        "    save_weights_only=False,  # Save the entire model (architecture + weights)\n",
        "    mode='min',  # Save the model when validation loss is minimum\n",
        "    verbose=1  # Print a message when saving the best model\n",
        ")\n",
        "\n",
        "# 9. Train the model and save the best model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_smiles], y_train,\n",
        "    epochs=100,\n",
        "    batch_size=200,\n",
        "    validation_data=([X_test_seq, X_test_smiles], y_test),\n",
        "    callbacks=[checkpoint_callback]  # Use checkpoint callback to save best model\n",
        ")\n",
        "\n",
        "# 10. Evaluate the model\n",
        "test_loss, test_acc = model.evaluate([X_test_seq, X_test_smiles], y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# Optionally, plot training history (accuracy and loss)\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Save the training history to a file\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.to_csv('training_history.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ufmxYaSNG5lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define your functions to preprocess the protein sequence and SMILES\n",
        "def one_hot_encoding(protein_seq, max_seq_len=5038):\n",
        "    amino_acids = 'ACDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    aa_dict = {amino_acids[i]: i for i in range(len(amino_acids))}\n",
        "    num_aa = len(aa_dict)\n",
        "\n",
        "    # One-hot encode the protein sequence\n",
        "    one_hot = np.zeros((1, max_seq_len, num_aa))  # Shape (1, max_seq_len, num_aa)\n",
        "\n",
        "    for j, aa in enumerate(protein_seq):\n",
        "        if aa in aa_dict:\n",
        "            one_hot[0, j, aa_dict[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Define the function to preprocess SMILES string (use the same as during training)\n",
        "def smiles_to_int(smiles, max_length=1000):\n",
        "    char_set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789()=#[]+-'\n",
        "    char_dict = {char: i+1 for i, char in enumerate(char_set)}\n",
        "\n",
        "    smiles_int = np.zeros((1, max_length), dtype=int)\n",
        "\n",
        "    for j, char in enumerate(smiles):\n",
        "        if j < max_length:\n",
        "            smiles_int[0, j] = char_dict.get(char, 0)\n",
        "    return smiles_int\n",
        "\n",
        "# Assuming `model` is already loaded and trained\n",
        "\n",
        "# Sample protein sequence to predict SMILES (for example)\n",
        "protein_sequence = \"MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYGAEEMPTLNRRAKQL\"\n",
        "# Step 1: Preprocess the input protein sequence\n",
        "X_seq = one_hot_encoding(protein_sequence)\n",
        "\n",
        "# Step 2: Make prediction using the model\n",
        "# Note: We're passing the protein sequence and dummy SMILES input (because the model is expecting both)\n",
        "dummy_smiles = np.zeros((1, 1000))  # Dummy input for SMILES (since it's a multi-input model)\n",
        "predicted_smiles_int = model.predict([X_seq, dummy_smiles])\n",
        "\n",
        "# Step 3: Convert predicted SMILES from integer to character string\n",
        "def int_to_smiles(smiles_int):\n",
        "    char_set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789()=#[]+-'\n",
        "    char_dict = {i+1: char for i, char in enumerate(char_set)}  # Reverse mapping\n",
        "\n",
        "    smiles = \"\"\n",
        "    for i in range(smiles_int.shape[1]):  # 1000 characters\n",
        "        char_idx = smiles_int[0, i]\n",
        "        if char_idx != 0:  # Skip padding (0 represents padding)\n",
        "            smiles += char_dict.get(char_idx, '')\n",
        "    return smiles\n",
        "\n",
        "# Step 4: Convert the predicted SMILES integers back to string\n",
        "predicted_smiles = int_to_smiles(predicted_smiles_int)\n",
        "\n",
        "print(\"Predicted SMILES:\", predicted_smiles)\n"
      ],
      "metadata": {
        "id": "5z75y788A2cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MULTIPLE SMILE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('final_output_15_2_25.csv')\n",
        "\n",
        "# Extract necessary columns\n",
        "protein_sequences = data['Sequence'].values\n",
        "smiles_strings = data['SMILE'].values\n",
        "\n",
        "# 1. Preprocess protein sequences (One-hot encoding)\n",
        "def one_hot_encoding(protein_seq):\n",
        "    amino_acids = 'ACDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    aa_dict = {amino_acids[i]: i for i in range(len(amino_acids))}\n",
        "    num_aa = len(aa_dict)\n",
        "\n",
        "    max_seq_len = 5038  # Maximum protein sequence length\n",
        "    one_hot = np.zeros((len(protein_seq), max_seq_len, num_aa))  # Pad all sequences to max length\n",
        "\n",
        "    for i, seq in enumerate(protein_seq):\n",
        "        for j, aa in enumerate(seq):\n",
        "            if aa in aa_dict:\n",
        "                one_hot[i, j, aa_dict[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Apply one-hot encoding to protein sequences\n",
        "X_seq = one_hot_encoding(protein_sequences)\n",
        "\n",
        "# 2. Preprocess SMILES strings (Integer encoding)\n",
        "def smiles_to_int(smiles_strings, max_length=1000):\n",
        "    char_set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789()=#[]+-'\n",
        "    char_dict = {char: i+1 for i, char in enumerate(char_set)}  # Map chars to integers\n",
        "\n",
        "    smiles_int = np.zeros((len(smiles_strings), max_length), dtype=int)\n",
        "\n",
        "    for i, smile in enumerate(smiles_strings):\n",
        "        for j, char in enumerate(smile):\n",
        "            if j < max_length:\n",
        "                smiles_int[i, j] = char_dict.get(char, 0)  # Map characters to integers\n",
        "    return smiles_int\n",
        "\n",
        "# Apply integer encoding to SMILES strings\n",
        "X_smiles = smiles_to_int(smiles_strings)\n",
        "\n",
        "# 3. Split the data into training and test sets\n",
        "X_train_seq, X_test_seq, X_train_smiles, X_test_smiles = train_test_split(\n",
        "    X_seq, X_smiles, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Pad the SMILES sequences (to 1000)\n",
        "X_train_smiles = pad_sequences(X_train_smiles, maxlen=1000, padding='post')\n",
        "X_test_smiles = pad_sequences(X_test_smiles, maxlen=1000, padding='post')\n",
        "\n",
        "# 5. Prepare target variable (assuming y is your target, with 1000 classes)\n",
        "y_train = np.random.randint(0, 2, size=(len(X_train_smiles), 1000))  # Example of multi-label target\n",
        "y_test = np.random.randint(0, 2, size=(len(X_test_smiles), 1000))  # Example of multi-label target\n",
        "\n",
        "# 6. Modify the model output for multi-label classification\n",
        "# Protein sequence model (1D Conv)\n",
        "seq_input = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (5038, 25)\n",
        "x = Conv1D(64, 3, activation='relu')(seq_input)\n",
        "x = MaxPooling1D(2)(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "# SMILES sequence model (Dense)\n",
        "smiles_input = Input(shape=(X_train_smiles.shape[1],))  # (1000,)\n",
        "y = Dense(128, activation='relu')(smiles_input)\n",
        "\n",
        "# Merge both models\n",
        "merged = tf.keras.layers.concatenate([x, y])\n",
        "\n",
        "# Output layer for multi-label classification\n",
        "output = Dense(1000, activation='sigmoid')(merged)  # Sigmoid for multi-label classification\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[seq_input, smiles_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# 7. Set up Model Checkpoint to save the best model during training\n",
        "checkpoint_dir = './model_checkpoints'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'best_model.h5'),\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 8. Train the model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_smiles], y_train,\n",
        "    epochs=1,\n",
        "    batch_size=2,\n",
        "    validation_data=([X_test_seq, X_test_smiles], y_test),\n",
        "    callbacks=[checkpoint_callback]\n",
        ")\n",
        "\n",
        "# 9. Evaluate the model\n",
        "test_loss, test_acc = model.evaluate([X_test_seq, X_test_smiles], y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# 10. Plot training history (optional)\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Save the training history to a file\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.to_csv('training_history.csv', index=False)\n"
      ],
      "metadata": {
        "id": "T17Rj6DoBz70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MULTIPLE PREDICTION\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "\n",
        "# Load your dataset (replace with your actual file path)\n",
        "data = pd.read_csv('final_output_15_2_25.csv')\n",
        "\n",
        "# Assuming the dataset has a column 'Protein_Sequence' containing protein sequences\n",
        "protein_sequences = data['Protein_Sequence'].values  # Protein sequences\n",
        "\n",
        "# 1. Preprocess Protein Sequences (Integer encoding)\n",
        "def protein_to_int(protein_sequences, max_length=1000):\n",
        "    # Define the amino acid alphabet (20 standard amino acids)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWXY'  # May include non-standard amino acids (e.g., 'B', 'Z', etc.)\n",
        "    aa_dict = {aa: i+1 for i, aa in enumerate(amino_acids)}  # Map amino acids to integers\n",
        "\n",
        "    # Prepare the protein sequence array (initially set to zero)\n",
        "    protein_int = np.zeros((len(protein_sequences), max_length), dtype=int)\n",
        "\n",
        "    for i, sequence in enumerate(protein_sequences):\n",
        "        for j, aa in enumerate(sequence):\n",
        "            if j < max_length:\n",
        "                protein_int[i, j] = aa_dict.get(aa, 0)  # Map amino acids to integers\n",
        "    return protein_int\n",
        "\n",
        "# Apply integer encoding to protein sequences\n",
        "X_proteins = protein_to_int(protein_sequences)\n",
        "\n",
        "# 2. Pad the protein sequences (to 1000, or whatever max_length you prefer)\n",
        "max_length = 1000\n",
        "X_proteins = pad_sequences(X_proteins, maxlen=max_length, padding='post')\n",
        "\n",
        "# 3. Parameters for the VAE model\n",
        "latent_dim = 256  # Dimensionality of the latent space\n",
        "\n",
        "# Encoder Model\n",
        "inputs = layers.Input(shape=(max_length,))\n",
        "embedding = layers.Embedding(input_dim=256, output_dim=128)(inputs)\n",
        "x = layers.LSTM(256, return_sequences=False)(embedding)\n",
        "\n",
        "# Latent space (mean and log variance for the VAE)\n",
        "z_mean = layers.Dense(latent_dim)(x)\n",
        "z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "# Sampling function\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "# Decoder Model\n",
        "latent_inputs = layers.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(256, activation='relu')(latent_inputs)\n",
        "x = layers.RepeatVector(max_length)(x)\n",
        "x = layers.LSTM(256, return_sequences=True)(x)\n",
        "outputs = layers.TimeDistributed(layers.Dense(256, activation='softmax'))(x)\n",
        "\n",
        "# Instantiate the VAE Model\n",
        "vae = Model(inputs, outputs)\n",
        "vae.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "vae.summary()\n",
        "\n",
        "# 4. Set up Model Checkpoint to save the best model during training\n",
        "checkpoint_dir = './model_checkpoints'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'best_model.h5'),  # Save best model as .h5 file\n",
        "    monitor='loss',  # Monitor training loss (you can also monitor 'val_loss')\n",
        "    save_best_only=True,  # Save the best model only\n",
        "    save_weights_only=False,  # Save the entire model (architecture + weights)\n",
        "    mode='min',  # Save the model when the loss is minimum\n",
        "    verbose=1  # Print a message when saving the best model\n",
        ")\n",
        "\n",
        "# 5. Set up CSVLogger to save the training history\n",
        "history_csv_file = './training_history.csv'\n",
        "csv_logger = CSVLogger(history_csv_file, append=True)\n",
        "\n",
        "# 6. Train the model\n",
        "history = vae.fit(\n",
        "    X_proteins, X_proteins,  # We use protein sequences as both input and output\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    callbacks=[checkpoint_callback, csv_logger]  # Use both callbacks\n",
        ")\n",
        "\n",
        "# 7. Save final model after training\n",
        "vae.save('./vae_final_model.h5')  # Save the final model after training\n",
        "\n",
        "# 8. Optionally, you can load the model later using:\n",
        "# vae = tf.keras.models.load_model('./vae_final_model.h5')\n",
        "\n",
        "# 9. Training history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.to_csv('training_history.csv', index=False)\n",
        "\n",
        "# Optionally, plot training history (accuracy and loss)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZqwtVE-QKbbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PREDICTION MULTIPLE\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "\n",
        "# Load the trained VAE model\n",
        "vae = tf.keras.models.load_model('./vae_final_model.h5')\n",
        "\n",
        "# Function to encode protein sequences into integers\n",
        "def protein_to_int(protein_sequences, max_length=1000):\n",
        "    # Define the amino acid alphabet (20 standard amino acids)\n",
        "    amino_acids = 'ACDEFGHIKLMNPQRSTVWXY'  # May include non-standard amino acids (e.g., 'B', 'Z', etc.)\n",
        "    aa_dict = {aa: i+1 for i, aa in enumerate(amino_acids)}  # Map amino acids to integers\n",
        "\n",
        "    # Prepare the protein sequence array (initially set to zero)\n",
        "    protein_int = np.zeros((len(protein_sequences), max_length), dtype=int)\n",
        "\n",
        "    for i, sequence in enumerate(protein_sequences):\n",
        "        for j, aa in enumerate(sequence):\n",
        "            if j < max_length:\n",
        "                protein_int[i, j] = aa_dict.get(aa, 0)  # Map amino acids to integers\n",
        "    return protein_int\n",
        "\n",
        "# Function to generate SMILES from protein sequence\n",
        "def generate_smiles(protein_sequence, vae, max_length=1000, latent_dim=256):\n",
        "    # Step 1: Preprocess the protein sequence (encode and pad)\n",
        "    protein_int = protein_to_int([protein_sequence], max_length)\n",
        "    protein_int = pad_sequences(protein_int, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Step 2: Pass the protein sequence through the encoder part of the VAE to get the latent vector\n",
        "    encoder = tf.keras.models.Model(inputs=vae.input, outputs=vae.get_layer('lambda').output)  # Encoder part\n",
        "    z_mean, z_log_var = encoder(protein_int)\n",
        "\n",
        "    # Step 3: Sample the latent vector\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
        "    z = z_mean + tf.exp(0.5 * z_log_var) * epsilon  # Sample latent vector using the reparameterization trick\n",
        "\n",
        "    # Step 4: Decode the latent vector to generate SMILES (decoder part)\n",
        "    decoder = tf.keras.models.Model(inputs=vae.get_layer('lambda').input, outputs=vae.output)  # Decoder part\n",
        "    smiles_output = decoder(z)\n",
        "\n",
        "    # Step 5: Convert the generated SMILES to readable format (integer to character mapping)\n",
        "    smiles_generated = smiles_output.numpy().argmax(axis=-1)[0]  # Get the most probable character for each position\n",
        "\n",
        "    # Convert integers back to SMILES characters\n",
        "    char_set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789()=#[]+-'\n",
        "    int_to_char = {i+1: char for i, char in enumerate(char_set)}  # Reverse mapping from integer to character\n",
        "\n",
        "    smiles = ''.join([int_to_char.get(i, '') for i in smiles_generated])  # Join the characters into the SMILES string\n",
        "    return smiles\n",
        "\n",
        "# Example: Predict SMILES for a protein sequence\n",
        "protein_sequence = 'MKTIIALSYIFCLVFA'  # Replace this with your input protein sequence\n",
        "predicted_smiles = generate_smiles(protein_sequence, vae)\n",
        "\n",
        "print(f\"Predicted SMILES for the protein sequence '{protein_sequence}': {predicted_smiles}\")\n"
      ],
      "metadata": {
        "id": "_GJq8xKpLsWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Check for TPU and initialize TPU strategy if available\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect TPU\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)  # Correct initialization method\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    print(\"TPU initialized\")\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.get_strategy()  # Use default strategy if no TPU is found\n",
        "    print(\"No TPU detected, using default strategy (CPU/GPU)\")\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('updated_final_output_MORGAN.csv')\n",
        "\n",
        "# Extract protein sequences and Morgan Fingerprints\n",
        "protein_sequences = data['Sequence'].values\n",
        "\n",
        "# Handle NaN and ensure all Morgan Fingerprint values are strings\n",
        "data['Morgan_Fingerprint'] = data['Morgan_Fingerprint'].fillna('')  # Replace NaNs with empty strings\n",
        "data['Morgan_Fingerprint'] = data['Morgan_Fingerprint'].astype(str)  # Ensure all values are strings\n",
        "\n",
        "# Convert Morgan Fingerprints from string to integer arrays\n",
        "morgan_fingerprints = data['Morgan_Fingerprint'].apply(lambda x: np.array([int(bit) for bit in x.split(',') if bit.isdigit()])).values\n",
        "\n",
        "# Calculate max sequence length for both protein sequences and Morgan fingerprints\n",
        "max_seq_len = max([len(seq) for seq in protein_sequences])  # Maximum protein sequence length\n",
        "max_fp_len = max([len(fp) for fp in morgan_fingerprints])  # Maximum Morgan fingerprint length\n",
        "\n",
        "print(f\"Max protein sequence length: {max_seq_len}\")\n",
        "print(f\"Max Morgan fingerprint length: {max_fp_len}\")\n",
        "\n",
        "# 1. Preprocess protein sequences (One-hot encoding)\n",
        "def one_hot_encoding(protein_seq, max_seq_len):\n",
        "    amino_acids = 'ACDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    aa_dict = {amino_acids[i]: i for i in range(len(amino_acids))}\n",
        "    num_aa = len(aa_dict)\n",
        "\n",
        "    one_hot = np.zeros((len(protein_seq), max_seq_len, num_aa))  # Pad all sequences to max length\n",
        "\n",
        "    for i, seq in enumerate(protein_seq):\n",
        "        for j, aa in enumerate(seq):\n",
        "            if aa in aa_dict:\n",
        "                one_hot[i, j, aa_dict[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Apply one-hot encoding to protein sequences\n",
        "X_seq = one_hot_encoding(protein_sequences, max_seq_len)\n",
        "\n",
        "# 2. No padding for Morgan fingerprints (directly use them as output)\n",
        "y = np.array(morgan_fingerprints)  # Morgan fingerprints as output\n",
        "\n",
        "# Convert y to a proper Numpy array with numeric dtype\n",
        "y = np.array([np.array(fp, dtype=np.int32) for fp in y])\n",
        "\n",
        "# 3. Split the data into training and test sets\n",
        "X_train_seq, X_test_seq, y_train, y_test = train_test_split(\n",
        "    X_seq, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Build the model inside the strategy scope\n",
        "with strategy.scope():\n",
        "    # Protein sequence model (1D Conv)\n",
        "    seq_input = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (5038, 25)\n",
        "    x = Conv1D(64, 3, activation='relu')(seq_input)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Additional hidden layer\n",
        "    x = Dense(128, activation='relu')(x)  # New hidden layer with 128 units and ReLU activation\n",
        "\n",
        "    # Output layer for binary classification (per bit of fingerprint)\n",
        "    output = Dense(max_fp_len, activation='sigmoid')(x)  # 1024 output units for binary classification (0 or 1)\n",
        "\n",
        "    # Compile the model with binary cross-entropy loss\n",
        "    model = Model(inputs=seq_input, outputs=output)\n",
        "    model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Display model summary\n",
        "    model.summary()\n",
        "\n",
        "\n",
        "\n",
        "# 5. Set up Model Checkpoint and Early Stopping to save the best model during training\n",
        "checkpoint_dir = '/content'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'best_model.h5'),  # Save best model as .h5 file\n",
        "    monitor='accuracy',  # Monitor validation loss\n",
        "    save_best_only=True,  # Save the best model only\n",
        "    save_weights_only=False,  # Save the entire model (architecture + weights)\n",
        "    mode='min',  # Save the model when validation loss is minimum\n",
        "    verbose=1  # Print a message when saving the best model\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=5,  # Stop training if validation loss doesn't improve for 5 epochs\n",
        "    restore_best_weights=True,  # Restore best weights after stopping\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. Train the model and save the best model\n",
        "history = model.fit(\n",
        "    X_train_seq, y_train,\n",
        "    epochs=2,  # Train for more epochs (increased from 1)\n",
        "    batch_size=8,  # Increased batch size for better training\n",
        "    validation_data=(X_test_seq, y_test),\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback]  # Use checkpoint and early stopping\n",
        ")\n",
        "\n",
        "# 7. Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test_seq, y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# Optionally, plot training history (accuracy and loss)\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Save the training history to a file\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.to_csv('training_history.csv', index=False)\n"
      ],
      "metadata": {
        "id": "4Kc6CmCxnYPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7019b209-f19e-4dc8-9cd2-29fc7f5578e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No TPU detected, using default strategy (CPU/GPU)\n",
            "Max protein sequence length: 5038\n",
            "Max Morgan fingerprint length: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PREDICTION SEQUENCE TO MORGAN FINGERPRINT TO SMILE (MULTIPLE SMILE)\n",
        "!pip install rdkit\n",
        "from tensorflow.keras.models import load_model\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import DataStructs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load trained model\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "# Load dataset for SMILES reference\n",
        "data = pd.read_csv(\"updated_final_output_MORGAN.csv\")\n",
        "reference_smiles = data['SMILE'].values\n",
        "reference_fingerprints = np.stack([np.array([int(bit) for bit in x.split(',')]) for x in data['Morgan_Fingerprint'].values])\n",
        "\n",
        "# Function to find closest SMILES match based on similarity threshold range\n",
        "def get_closest_smiles(predicted_fp, min_similarity=0.7, max_similarity=0.9):\n",
        "    matches = []  # List to store SMILES that meet the similarity criteria\n",
        "    for i, ref_fp in enumerate(reference_fingerprints):\n",
        "        similarity = DataStructs.FingerprintSimilarity(\n",
        "            Chem.DataStructs.CreateFromBitString(''.join(map(str, predicted_fp.astype(int)))),\n",
        "            Chem.DataStructs.CreateFromBitString(''.join(map(str, ref_fp.astype(int))))\n",
        "        )\n",
        "        # If similarity is within the given range, add it to the matches\n",
        "        if min_similarity <= similarity <= max_similarity:\n",
        "            matches.append((reference_smiles[i], similarity))\n",
        "\n",
        "    # Sort matches by similarity score (optional)\n",
        "    matches.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Function to predict multiple SMILES based on protein sequence and similarity range\n",
        "def predict_multiple_smiles(protein_sequence, min_similarity=0.7, max_similarity=0.9):\n",
        "    # Perform one-hot encoding of the protein sequence (ensure max_seq_len is defined)\n",
        "    one_hot_seq = one_hot_encoding([protein_sequence], max_seq_len)  # Adjust max_seq_len\n",
        "    predicted_fp = model.predict(one_hot_seq)[0]  # Get predicted fingerprint\n",
        "\n",
        "    # Get multiple SMILES predictions based on similarity range\n",
        "    closest_matches = get_closest_smiles(predicted_fp, min_similarity, max_similarity)\n",
        "\n",
        "    return closest_matches\n",
        "\n",
        "# Example protein sequence input (change this to your input)\n",
        "protein_input = \"MGNASNDSQSEDCETRQWLPPGESPAISSVMFSAGVLGNLIALALLARRWRGDVGCSAGRRSSLSLFHVLVTELVFTDLLGTCLISPVVLASYARNQTLVALAPESRACTYFAFAMTFFSLATMLMLFAMALERYLSIGHPYFYQRRVSRSGGLAVLPVIYAVSLLFCSLPLLDYGQYVQYCPGTWCFIRHGRTAYLQLYATLLLLLIVSVLACNFSVILNLIRMHRRSRRSRCGPSLGSGRGGPGARRRGERVSMAEETDHLILLAIMTITFAVCSLPFTIFAYMNETSSRKEKWDLQALRFLSINSIIDPWVFAILRPPVLRLMRSVLCCRISLRTQDATQTSCSTQSDASKQADL\"\n",
        "\n",
        "# Predict multiple SMILES with similarity above 0.7 and below 0.9\n",
        "predicted_matches = predict_multiple_smiles(protein_input, min_similarity=0.7, max_similarity=0.9)\n",
        "\n",
        "# Display the predictions\n",
        "for smiles, similarity in predicted_matches:\n",
        "    print(f\"Predicted SMILES: {smiles}, Similarity: {similarity}\")\n"
      ],
      "metadata": {
        "id": "5qVXUhWincVy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "6f0ed68d-7319-4dd8-be42-bf2c5d570bd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.9.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
            "Downloading rdkit-2024.9.5-cp311-cp311-manylinux_2_28_x86_64.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.9.5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'best_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6c8179c96f9b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load dataset for SMILES reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    559\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'best_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YEfRRx96BQG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}