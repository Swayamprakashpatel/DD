{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOuRN/kYJyCUB5FW43n/nep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swayamprakashpatel/DD/blob/main/Drug_Discovery_15_2_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGjllWymy5bL",
        "outputId": "3b81ad0a-a375-4dbf-8f3d-2bf3d482f195"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('final_output_15_2_25.csv')\n",
        "\n",
        "# Extract necessary columns\n",
        "protein_sequences = data['Sequence'].values\n",
        "smiles_strings = data['SMILE'].values\n",
        "\n",
        "# 1. Preprocess protein sequences (One-hot encoding)\n",
        "def one_hot_encoding(protein_seq):\n",
        "    amino_acids = 'ACDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    aa_dict = {amino_acids[i]: i for i in range(len(amino_acids))}\n",
        "    num_aa = len(aa_dict)\n",
        "\n",
        "    max_seq_len = 5038  # Maximum protein sequence length\n",
        "    one_hot = np.zeros((len(protein_seq), max_seq_len, num_aa))  # Pad all sequences to max length\n",
        "\n",
        "    for i, seq in enumerate(protein_seq):\n",
        "        for j, aa in enumerate(seq):\n",
        "            if aa in aa_dict:\n",
        "                one_hot[i, j, aa_dict[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Apply one-hot encoding to protein sequences\n",
        "X_seq = one_hot_encoding(protein_sequences)\n",
        "\n",
        "# 2. Preprocess SMILES strings (Integer encoding)\n",
        "def smiles_to_int(smiles_strings, max_length=1000):\n",
        "    char_set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789()=#[]+-'\n",
        "    char_dict = {char: i+1 for i, char in enumerate(char_set)}  # Map chars to integers\n",
        "\n",
        "    smiles_int = np.zeros((len(smiles_strings), max_length), dtype=int)\n",
        "\n",
        "    for i, smile in enumerate(smiles_strings):\n",
        "        for j, char in enumerate(smile):\n",
        "            if j < max_length:\n",
        "                smiles_int[i, j] = char_dict.get(char, 0)  # Map characters to integers\n",
        "    return smiles_int\n",
        "\n",
        "# Apply integer encoding to SMILES strings\n",
        "X_smiles = smiles_to_int(smiles_strings)\n",
        "\n",
        "# 3. Split the data into training and test sets\n",
        "X_train_seq, X_test_seq, X_train_smiles, X_test_smiles = train_test_split(\n",
        "    X_seq, X_smiles, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Pad the SMILES sequences (to 1000)\n",
        "X_train_smiles = pad_sequences(X_train_smiles, maxlen=1000, padding='post')\n",
        "X_test_smiles = pad_sequences(X_test_smiles, maxlen=1000, padding='post')\n",
        "\n",
        "# 5. Prepare target variable (assuming y is your target, with 1000 classes)\n",
        "y_train = np.zeros(len(X_train_smiles))  # Dummy binary target variable\n",
        "y_test = np.zeros(len(X_test_smiles))  # Dummy binary target variable\n",
        "\n",
        "# 6. One-hot encode the target variable\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=1000)  # One-hot encoding\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=1000)\n",
        "\n",
        "# 7. Build the model\n",
        "\n",
        "# Protein sequence model (1D Conv)\n",
        "seq_input = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (5038, 25)\n",
        "x = Conv1D(64, 3, activation='relu')(seq_input)\n",
        "x = MaxPooling1D(2)(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "# SMILES sequence model (Dense)\n",
        "smiles_input = Input(shape=(X_train_smiles.shape[1],))  # (1000,)\n",
        "y = Dense(128, activation='relu')(smiles_input)\n",
        "\n",
        "# Merge both models\n",
        "merged = tf.keras.layers.concatenate([x, y])\n",
        "\n",
        "# Output layer for multi-class classification\n",
        "output = Dense(1000, activation='softmax')(merged)  # Output for 1000 classes\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[seq_input, smiles_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# 8. Train the model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_smiles], y_train,\n",
        "    epochs=100,\n",
        "    batch_size=200,\n",
        "    validation_data=([X_test_seq, X_test_smiles], y_test)\n",
        ")\n",
        "\n",
        "# 9. Evaluate the model\n",
        "test_loss, test_acc = model.evaluate([X_test_seq, X_test_smiles], y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# Optionally, plot training history (accuracy and loss)\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "WxzzxufZAa95",
        "outputId": "ae264259-a25c-4976-8b63-1d191aa25005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5038\u001b[0m, \u001b[38;5;34m25\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5036\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │          \u001b[38;5;34m4,864\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2518\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161152\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m128,128\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161280\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
              "│                           │                        │                │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │    \u001b[38;5;34m161,281,000\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5038</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5036</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ max_pooling1d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2518</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161152</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128,128</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161280</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
              "│                           │                        │                │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">161,281,000</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m161,413,992\u001b[0m (615.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,413,992</span> (615.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m161,413,992\u001b[0m (615.75 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,413,992</span> (615.75 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1358s\u001b[0m 11s/step - accuracy: 0.9573 - loss: 0.3992 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\u001b[1m 13/126\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18:44\u001b[0m 10s/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('final_output_15_2_25.csv')\n",
        "\n",
        "# Extract necessary columns\n",
        "protein_sequences = data['Sequence'].values\n",
        "smiles_strings = data['SMILE'].values\n",
        "\n",
        "# 1. Preprocess protein sequences (One-hot encoding)\n",
        "def one_hot_encoding(protein_seq):\n",
        "    amino_acids = 'ACDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    aa_dict = {amino_acids[i]: i for i in range(len(amino_acids))}\n",
        "    num_aa = len(aa_dict)\n",
        "\n",
        "    max_seq_len = 5038  # Maximum protein sequence length\n",
        "    one_hot = np.zeros((len(protein_seq), max_seq_len, num_aa))  # Pad all sequences to max length\n",
        "\n",
        "    for i, seq in enumerate(protein_seq):\n",
        "        for j, aa in enumerate(seq):\n",
        "            if aa in aa_dict:\n",
        "                one_hot[i, j, aa_dict[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Apply one-hot encoding to protein sequences\n",
        "X_seq = one_hot_encoding(protein_sequences)\n",
        "\n",
        "# 2. Preprocess SMILES strings (Integer encoding)\n",
        "def smiles_to_int(smiles_strings, max_length=1000):\n",
        "    char_set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789()=#[]+-'\n",
        "    char_dict = {char: i+1 for i, char in enumerate(char_set)}  # Map chars to integers\n",
        "\n",
        "    smiles_int = np.zeros((len(smiles_strings), max_length), dtype=int)\n",
        "\n",
        "    for i, smile in enumerate(smiles_strings):\n",
        "        for j, char in enumerate(smile):\n",
        "            if j < max_length:\n",
        "                smiles_int[i, j] = char_dict.get(char, 0)  # Map characters to integers\n",
        "    return smiles_int\n",
        "\n",
        "# Apply integer encoding to SMILES strings\n",
        "X_smiles = smiles_to_int(smiles_strings)\n",
        "\n",
        "# 3. Split the data into training and test sets\n",
        "X_train_seq, X_test_seq, X_train_smiles, X_test_smiles = train_test_split(\n",
        "    X_seq, X_smiles, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Pad the SMILES sequences (to 1000)\n",
        "X_train_smiles = pad_sequences(X_train_smiles, maxlen=1000, padding='post')\n",
        "X_test_smiles = pad_sequences(X_test_smiles, maxlen=1000, padding='post')\n",
        "\n",
        "# 5. Prepare target variable (assuming y is your target, with 1000 classes)\n",
        "y_train = np.zeros(len(X_train_smiles))  # Dummy binary target variable\n",
        "y_test = np.zeros(len(X_test_smiles))  # Dummy binary target variable\n",
        "\n",
        "# 6. One-hot encode the target variable\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=1000)  # One-hot encoding\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=1000)\n",
        "\n",
        "# 7. Build the model\n",
        "\n",
        "# Protein sequence model (1D Conv)\n",
        "seq_input = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))  # (5038, 25)\n",
        "x = Conv1D(64, 3, activation='relu')(seq_input)\n",
        "x = MaxPooling1D(2)(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "# SMILES sequence model (Dense)\n",
        "smiles_input = Input(shape=(X_train_smiles.shape[1],))  # (1000,)\n",
        "y = Dense(128, activation='relu')(smiles_input)\n",
        "\n",
        "# Merge both models\n",
        "merged = tf.keras.layers.concatenate([x, y])\n",
        "\n",
        "# Output layer for multi-class classification\n",
        "output = Dense(1000, activation='softmax')(merged)  # Output for 1000 classes\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[seq_input, smiles_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# 8. Set up Model Checkpoint to save the best model during training\n",
        "checkpoint_dir = './model_checkpoints'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, 'best_model.h5'),  # Save best model as .h5 file\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    save_best_only=True,  # Save the best model only\n",
        "    save_weights_only=False,  # Save the entire model (architecture + weights)\n",
        "    mode='min',  # Save the model when validation loss is minimum\n",
        "    verbose=1  # Print a message when saving the best model\n",
        ")\n",
        "\n",
        "# 9. Train the model and save the best model\n",
        "history = model.fit(\n",
        "    [X_train_seq, X_train_smiles], y_train,\n",
        "    epochs=100,\n",
        "    batch_size=200,\n",
        "    validation_data=([X_test_seq, X_test_smiles], y_test),\n",
        "    callbacks=[checkpoint_callback]  # Use checkpoint callback to save best model\n",
        ")\n",
        "\n",
        "# 10. Evaluate the model\n",
        "test_loss, test_acc = model.evaluate([X_test_seq, X_test_smiles], y_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# Optionally, plot training history (accuracy and loss)\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Save the training history to a file\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.to_csv('training_history.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ufmxYaSNG5lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define your functions to preprocess the protein sequence and SMILES\n",
        "def one_hot_encoding(protein_seq, max_seq_len=5038):\n",
        "    amino_acids = 'ACDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "    aa_dict = {amino_acids[i]: i for i in range(len(amino_acids))}\n",
        "    num_aa = len(aa_dict)\n",
        "\n",
        "    # One-hot encode the protein sequence\n",
        "    one_hot = np.zeros((1, max_seq_len, num_aa))  # Shape (1, max_seq_len, num_aa)\n",
        "\n",
        "    for j, aa in enumerate(protein_seq):\n",
        "        if aa in aa_dict:\n",
        "            one_hot[0, j, aa_dict[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Define the function to preprocess SMILES string (use the same as during training)\n",
        "def smiles_to_int(smiles, max_length=1000):\n",
        "    char_set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789()=#[]+-'\n",
        "    char_dict = {char: i+1 for i, char in enumerate(char_set)}\n",
        "\n",
        "    smiles_int = np.zeros((1, max_length), dtype=int)\n",
        "\n",
        "    for j, char in enumerate(smiles):\n",
        "        if j < max_length:\n",
        "            smiles_int[0, j] = char_dict.get(char, 0)\n",
        "    return smiles_int\n",
        "\n",
        "# Assuming `model` is already loaded and trained\n",
        "\n",
        "# Sample protein sequence to predict SMILES (for example)\n",
        "protein_sequence = \"MTEYKLVVVGAGGVGKSALTIQLIQNHFVDEYGAEEMPTLNRRAKQL\"\n",
        "# Step 1: Preprocess the input protein sequence\n",
        "X_seq = one_hot_encoding(protein_sequence)\n",
        "\n",
        "# Step 2: Make prediction using the model\n",
        "# Note: We're passing the protein sequence and dummy SMILES input (because the model is expecting both)\n",
        "dummy_smiles = np.zeros((1, 1000))  # Dummy input for SMILES (since it's a multi-input model)\n",
        "predicted_smiles_int = model.predict([X_seq, dummy_smiles])\n",
        "\n",
        "# Step 3: Convert predicted SMILES from integer to character string\n",
        "def int_to_smiles(smiles_int):\n",
        "    char_set = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789()=#[]+-'\n",
        "    char_dict = {i+1: char for i, char in enumerate(char_set)}  # Reverse mapping\n",
        "\n",
        "    smiles = \"\"\n",
        "    for i in range(smiles_int.shape[1]):  # 1000 characters\n",
        "        char_idx = smiles_int[0, i]\n",
        "        if char_idx != 0:  # Skip padding (0 represents padding)\n",
        "            smiles += char_dict.get(char_idx, '')\n",
        "    return smiles\n",
        "\n",
        "# Step 4: Convert the predicted SMILES integers back to string\n",
        "predicted_smiles = int_to_smiles(predicted_smiles_int)\n",
        "\n",
        "print(\"Predicted SMILES:\", predicted_smiles)\n"
      ],
      "metadata": {
        "id": "5z75y788A2cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T17Rj6DoBz70"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}