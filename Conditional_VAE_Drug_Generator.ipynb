{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swayamprakashpatel/DD/blob/main/Conditional_VAE_Drug_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9EyueZqCNYB",
        "outputId": "c399ff35-e74d-45f8-f698-7190580d28d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.12/dist-packages (2025.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "Successfully installed sympy-1.14.0\n",
            "Collecting sympy==1.12\n",
            "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.12/dist-packages (from sympy==1.12) (1.3.0)\n",
            "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Installing collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.8.0+cu126 requires sympy>=1.13.3, but you have sympy 1.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.12\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOUUhf10LxgR",
        "outputId": "fd2724a8-bcbf-4b75-a917-f78073140211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Starting training...\n",
            "--- Epoch 1/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.69it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Train Loss: 689.7815, Validation Loss: 265.7975\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 2/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.68it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/100, Train Loss: 253.8396, Validation Loss: 260.1969\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 3/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.69it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/100, Train Loss: 252.3118, Validation Loss: 260.8699\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 4/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.73it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/100, Train Loss: 251.5194, Validation Loss: 260.9197\n",
            "Validation loss did not improve. Patience: 2/5\n",
            "--- Epoch 5/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.71it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/100, Train Loss: 250.5495, Validation Loss: 258.9969\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 6/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.71it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/100, Train Loss: 250.4602, Validation Loss: 261.4875\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 7/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.73it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/100, Train Loss: 251.3456, Validation Loss: 262.8319\n",
            "Validation loss did not improve. Patience: 2/5\n",
            "--- Epoch 8/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.71it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/100, Train Loss: 250.2731, Validation Loss: 258.6402\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 9/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.70it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/100, Train Loss: 249.9694, Validation Loss: 257.8888\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 10/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.72it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/100, Train Loss: 249.9739, Validation Loss: 258.2702\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 11/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.72it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/100, Train Loss: 251.1656, Validation Loss: 262.5778\n",
            "Validation loss did not improve. Patience: 2/5\n",
            "--- Epoch 12/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.71it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/100, Train Loss: 249.9530, Validation Loss: 258.7560\n",
            "Validation loss did not improve. Patience: 3/5\n",
            "--- Epoch 13/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.73it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/100, Train Loss: 250.0808, Validation Loss: 252.2615\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 14/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.71it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/100, Train Loss: 243.3347, Validation Loss: 243.8691\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 15/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.73it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/100, Train Loss: 238.1384, Validation Loss: 239.2787\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 16/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.74it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16/100, Train Loss: 228.9487, Validation Loss: 234.6226\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 17/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.73it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17/100, Train Loss: 224.6879, Validation Loss: 230.7517\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 18/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.73it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18/100, Train Loss: 223.5139, Validation Loss: 234.1996\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 19/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.69it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/100, Train Loss: 221.9148, Validation Loss: 229.5290\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 20/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.72it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20/100, Train Loss: 220.2351, Validation Loss: 230.6503\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 21/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.72it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21/100, Train Loss: 205.8357, Validation Loss: 180.4476\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 22/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.73it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22/100, Train Loss: 163.2376, Validation Loss: 155.8979\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 23/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.73it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23/100, Train Loss: 151.7030, Validation Loss: 151.8654\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 24/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.72it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24/100, Train Loss: 149.7808, Validation Loss: 149.0854\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 25/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.73it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25/100, Train Loss: 147.6726, Validation Loss: 153.5390\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 26/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.73it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26/100, Train Loss: 146.3741, Validation Loss: 150.2032\n",
            "Validation loss did not improve. Patience: 2/5\n",
            "--- Epoch 27/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.76it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27/100, Train Loss: 146.4480, Validation Loss: 148.1743\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 28/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.72it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28/100, Train Loss: 146.1093, Validation Loss: 155.2575\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 29/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.70it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29/100, Train Loss: 144.7198, Validation Loss: 148.8429\n",
            "Validation loss did not improve. Patience: 2/5\n",
            "--- Epoch 30/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.71it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30/100, Train Loss: 144.0362, Validation Loss: 146.8843\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 31/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.71it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31/100, Train Loss: 143.7447, Validation Loss: 147.6457\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 32/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.62it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32/100, Train Loss: 142.7004, Validation Loss: 145.2394\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 33/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.67it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33/100, Train Loss: 142.3609, Validation Loss: 147.6154\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 34/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.67it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34/100, Train Loss: 141.4599, Validation Loss: 142.8907\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 35/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.65it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35/100, Train Loss: 141.6949, Validation Loss: 144.6898\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 36/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.65it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36/100, Train Loss: 140.9138, Validation Loss: 150.5206\n",
            "Validation loss did not improve. Patience: 2/5\n",
            "--- Epoch 37/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.69it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37/100, Train Loss: 140.5105, Validation Loss: 149.9910\n",
            "Validation loss did not improve. Patience: 3/5\n",
            "--- Epoch 38/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.68it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38/100, Train Loss: 139.9754, Validation Loss: 142.1477\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 39/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.65it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39/100, Train Loss: 140.0387, Validation Loss: 140.5258\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 40/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.65it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40/100, Train Loss: 139.3303, Validation Loss: 140.4829\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 41/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.68it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41/100, Train Loss: 138.0666, Validation Loss: 141.3265\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 42/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.68it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42/100, Train Loss: 138.4691, Validation Loss: 139.1720\n",
            "Validation loss improved. Saving model to best_model.pt\n",
            "--- Epoch 43/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.70it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43/100, Train Loss: 137.6594, Validation Loss: 143.4871\n",
            "Validation loss did not improve. Patience: 1/5\n",
            "--- Epoch 44/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.67it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44/100, Train Loss: 137.8318, Validation Loss: 141.5805\n",
            "Validation loss did not improve. Patience: 2/5\n",
            "--- Epoch 45/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.72it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45/100, Train Loss: 137.6625, Validation Loss: 141.8927\n",
            "Validation loss did not improve. Patience: 3/5\n",
            "--- Epoch 46/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:50<00:00,  7.71it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46/100, Train Loss: 137.2317, Validation Loss: 144.4704\n",
            "Validation loss did not improve. Patience: 4/5\n",
            "--- Epoch 47/100 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 393/393 [00:51<00:00,  7.68it/s]\n",
            "Validation: 100%|██████████| 99/99 [00:07<00:00, 13.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47/100, Train Loss: 137.8126, Validation Loss: 139.4465\n",
            "Validation loss did not improve. Patience: 5/5\n",
            "Early stopping triggered.\n",
            "\n",
            "Training complete. Model saved to 'best_model.pt'.\n",
            "\n",
            "Attempting to generate 5 molecules for protein sequence: MGNASNDSQSEDCETRQWLPPGESPAISSVMFSAGVLGNLIALALLARRWRGDVGCSAGRRSSLSLFHVLVTELVFTDLLGTCLISPVVLASYARNQTLVALAPESRACTYFAFAMTFFSLATMLMLFAMALERYLSIGHPYFYQRRVSRSGGLAVLPVIYAVSLLFCSLPLLDYGQYVQYCPGTWCFIRHGRTAYLQLYATLLLLLIVSVLACNFSVILNLIRMHRRSRRSRCGPSLGSGRGGPGARRRGERVSMAEETDHLILLAIMTITFAVCSLPFTIFAYMNETSSRKEKWDLQALRFLSINSIIDPWVFAILRPPVLRLMRSVLCCRISLRTQDATQTSCSTQSDASKQADL\n",
            "Generated molecule (valid SMILES): CC(=CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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\n",
            "image/png": "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\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated molecule (invalid SMILES): CN=CC=CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC))))O\n",
            "Generated molecule (valid SMILES): CNCCCCCCCCCCCCCCCCCCCCCC\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[11:54:46] SMILES Parse Error: extra close parentheses while parsing: CN=CC=CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC))))O\n",
            "[11:54:46] SMILES Parse Error: check for mistakes around position 38:\n",
            "[11:54:46] CCCCCCCCCCCCCCCCCCCC))))O\n",
            "[11:54:46] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[11:54:46] SMILES Parse Error: Failed parsing SMILES 'CN=CC=CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC))))O' for input: 'CN=CC=CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC))))O'\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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\n",
            "image/png": "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\n",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated molecule (invalid SMILES): COCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)\n",
            "Generated molecule (invalid SMILES): CC(=CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[11:54:46] SMILES Parse Error: extra close parentheses while parsing: COCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)\n",
            "[11:54:46] SMILES Parse Error: check for mistakes around position 56:\n",
            "[11:54:46] CCCCCCCCCCCCCCCCCCCC)\n",
            "[11:54:46] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[11:54:46] SMILES Parse Error: Failed parsing SMILES 'COCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)' for input: 'COCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC)'\n",
            "[11:54:46] SMILES Parse Error: extra open parentheses while parsing: CC(=CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
            "[11:54:46] SMILES Parse Error: check for mistakes around position 3:\n",
            "[11:54:46] CC(=CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
            "[11:54:46] ~~^\n",
            "[11:54:46] SMILES Parse Error: Failed parsing SMILES 'CC(=CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC' for input: 'CC(=CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.Draw import MolToImage\n",
        "from IPython.display import Image, display\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Suppress RDKit warnings for a cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Global constants for SMILES tokenization ---\n",
        "# This dictionary has been updated to include a padding character '`'\n",
        "# which is used to ensure all SMILES strings have a consistent length.\n",
        "SMILES_TOKEN_DICT = {\n",
        "    '#': 0, '(': 1, ')': 2, '+': 3, '-': 4, '/': 5, '=': 6, '[': 7, ']': 8,\n",
        "    'C': 9, 'c': 10, 'H': 11, 'N': 12, 'O': 13, 'S': 14, 'F': 15, 'I': 16,\n",
        "    'P': 17, 'n': 18, 's': 19, 'B': 20, 'Cl': 21, 'Br': 22, 'r': 23, 'l': 24,\n",
        "    'o': 25, 'p': 26, 'se': 27, 'Se': 28, 'a': 29, 'i': 30, '`': 31\n",
        "}\n",
        "\n",
        "SMILES_MAX_LEN = 1000\n",
        "HIDDEN_DIM = 256\n",
        "LATENT_DIM = 64\n",
        "NUM_LAYERS = 2\n",
        "MAX_EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "PATIENCE = 5\n",
        "LEARNING_RATE = 1e-3\n",
        "MODEL_SAVE_PATH = 'best_model.pt'\n",
        "\n",
        "# --- Data Preprocessing and PyTorch Dataset ---\n",
        "\n",
        "def smiles_to_one_hot(smiles, max_len=SMILES_MAX_LEN):\n",
        "    \"\"\"Encodes a SMILES string into a one-hot vector with padding.\"\"\"\n",
        "    token_to_idx = SMILES_TOKEN_DICT\n",
        "    if len(smiles) > max_len:\n",
        "        # Truncate and add end-of-string token\n",
        "        smiles = smiles[:max_len-1] + '`'\n",
        "    else:\n",
        "        # Pad with end-of-string token\n",
        "        smiles += '`' * (max_len - len(smiles))\n",
        "\n",
        "    one_hot_vector = np.zeros((max_len, len(token_to_idx)))\n",
        "    for i, token in enumerate(list(smiles)):\n",
        "        if token in token_to_idx:\n",
        "            one_hot_vector[i, token_to_idx[token]] = 1\n",
        "    return one_hot_vector\n",
        "\n",
        "def get_protein_embedding(seq):\n",
        "    \"\"\"\n",
        "    A simple, placeholder protein embedding function.\n",
        "    In a real-world scenario, you would use a more sophisticated method,\n",
        "    like a pre-trained protein language model.\n",
        "    \"\"\"\n",
        "    protein_vocab = {'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9, 'M': 10,\n",
        "                     'N': 11, 'P': 12, 'Q': 13, 'R': 14, 'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19}\n",
        "    embedding = np.zeros(len(protein_vocab))\n",
        "    for amino_acid in seq:\n",
        "        if amino_acid in protein_vocab:\n",
        "            embedding[protein_vocab[amino_acid]] = 1\n",
        "    return embedding\n",
        "\n",
        "class SmilesProteinDataset(Dataset):\n",
        "    \"\"\"Dataset for loading SMILES and Protein sequence pairs.\"\"\"\n",
        "    def __init__(self, smiles_list, protein_list):\n",
        "        self.smiles_list = smiles_list\n",
        "        self.protein_list = protein_list\n",
        "        # The protein embedding dimension is determined from the first sequence in the list\n",
        "        self.protein_embedding_dim = len(get_protein_embedding(self.protein_list[0]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        smiles = self.smiles_list[idx]\n",
        "        protein = self.protein_list[idx]\n",
        "        smiles_one_hot = smiles_to_one_hot(smiles)\n",
        "        protein_embedding = get_protein_embedding(protein)\n",
        "        return torch.tensor(smiles_one_hot, dtype=torch.float32), torch.tensor(protein_embedding, dtype=torch.float32)\n",
        "\n",
        "# --- Conditional VAE Model Definition ---\n",
        "\n",
        "class ConditionalVAE(nn.Module):\n",
        "    def __init__(self, smiles_vocab_size, protein_embedding_dim, hidden_dim, latent_dim, smiles_max_len, num_layers):\n",
        "        super(ConditionalVAE, self).__init__()\n",
        "\n",
        "        self.smiles_max_len = smiles_max_len\n",
        "        self.smiles_vocab_size = smiles_vocab_size\n",
        "        self.protein_embedding_dim = protein_embedding_dim\n",
        "\n",
        "        # Encoder: takes smiles and protein embedding as input\n",
        "        self.encoder_rnn = nn.GRU(\n",
        "            input_size=smiles_vocab_size,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder_protein_mlp = nn.Sequential(\n",
        "            nn.Linear(protein_embedding_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder: takes latent vector and protein embedding as input\n",
        "        self.decoder_gru_input_mlp = nn.Sequential(\n",
        "            nn.Linear(latent_dim + protein_embedding_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder_rnn = nn.GRU(\n",
        "            input_size=smiles_vocab_size,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc_output = nn.Linear(hidden_dim, smiles_vocab_size)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"Reparameterization trick to sample from N(mu, var)\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def encoder(self, smiles_one_hot, protein_embedding):\n",
        "        _, hidden_smiles = self.encoder_rnn(smiles_one_hot)\n",
        "        hidden_protein = self.encoder_protein_mlp(protein_embedding).unsqueeze(0).repeat(hidden_smiles.size(0), 1, 1)\n",
        "        combined_hidden = hidden_smiles + hidden_protein\n",
        "        final_hidden_state = combined_hidden[-1, :, :]\n",
        "        mu = self.fc_mu(final_hidden_state)\n",
        "        logvar = self.fc_logvar(final_hidden_state)\n",
        "        return mu, logvar\n",
        "\n",
        "    def decoder(self, z, protein_embedding):\n",
        "        z_with_protein = torch.cat((z, protein_embedding), dim=1)\n",
        "        initial_hidden = self.decoder_gru_input_mlp(z_with_protein).unsqueeze(0)\n",
        "        initial_hidden = initial_hidden.repeat(self.decoder_rnn.num_layers, 1, 1)\n",
        "        decoder_input = torch.zeros(z.size(0), self.smiles_max_len, self.smiles_vocab_size).to(z.device)\n",
        "        decoder_input[:, 0, SMILES_TOKEN_DICT['#']] = 1\n",
        "        output, _ = self.decoder_rnn(decoder_input, initial_hidden)\n",
        "        output = self.fc_output(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, smiles_one_hot, protein_embedding):\n",
        "        mu, logvar = self.encoder(smiles_one_hot, protein_embedding)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        reconstructed_smiles = self.decoder(z, protein_embedding)\n",
        "        return reconstructed_smiles, mu, logvar\n",
        "\n",
        "# --- Training Loop and Generation ---\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    \"\"\"VAE loss function: BCE + KL divergence.\"\"\"\n",
        "    # Ensure dimensions are compatible for BCE calculation\n",
        "    recon_x_flat = recon_x.view(-1, len(SMILES_TOKEN_DICT))\n",
        "    x_flat = x.view(-1, len(SMILES_TOKEN_DICT))\n",
        "    BCE = nn.functional.binary_cross_entropy_with_logits(recon_x_flat, x_flat, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "def train_model(model, dataloader, optimizer, device):\n",
        "    \"\"\"Trains the VAE model for one epoch with a progress bar.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for smiles_oh, protein_emb in tqdm(dataloader, desc=\"Training\"):\n",
        "        smiles_oh = smiles_oh.to(device)\n",
        "        protein_emb = protein_emb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        reconstructed_smiles, mu, logvar = model(smiles_oh, protein_emb)\n",
        "        loss = vae_loss(reconstructed_smiles, smiles_oh, mu, logvar)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    return avg_loss\n",
        "\n",
        "def validate_model(model, dataloader, device):\n",
        "    \"\"\"Evaluates the model on the validation set.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for smiles_oh, protein_emb in tqdm(dataloader, desc=\"Validation\"):\n",
        "            smiles_oh = smiles_oh.to(device)\n",
        "            protein_emb = protein_emb.to(device)\n",
        "\n",
        "            reconstructed_smiles, mu, logvar = model(smiles_oh, protein_emb)\n",
        "            loss = vae_loss(reconstructed_smiles, smiles_oh, mu, logvar)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    return avg_loss\n",
        "\n",
        "def generate_new_molecules(model, protein_seq, num_molecules=5, device=\"cpu\"):\n",
        "    \"\"\"Generates and visualizes new molecules for a given protein sequence.\"\"\"\n",
        "    model.eval()\n",
        "    idx_to_token = {v: k for k, v in SMILES_TOKEN_DICT.items()}\n",
        "    generated_molecules = []\n",
        "\n",
        "    # Prepare the protein embedding for a single sequence\n",
        "    protein_embedding = get_protein_embedding(protein_seq)\n",
        "    protein_embedding = torch.tensor(protein_embedding, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    print(f\"\\nAttempting to generate {num_molecules} molecules for protein sequence: {protein_seq}\")\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_molecules):\n",
        "            # Sample a vector from the latent space\n",
        "            z = torch.randn(1, LATENT_DIM).to(device)\n",
        "            # Use the decoder to generate a one-hot encoded SMILES string\n",
        "            generated_output = model.decoder(z, protein_embedding)\n",
        "\n",
        "            # Use softmax to get probabilities for each token and then select the most likely one\n",
        "            probabilities = nn.functional.softmax(generated_output, dim=-1)\n",
        "            predicted_indices = torch.argmax(probabilities, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "            generated_smiles = \"\"\n",
        "            for token_idx in predicted_indices:\n",
        "                token = idx_to_token.get(token_idx, '')\n",
        "                # Stop decoding when a padding token is encountered\n",
        "                if token == '`':\n",
        "                    break\n",
        "                generated_smiles += token\n",
        "\n",
        "            # Validate and display the generated SMILES string as an image\n",
        "            mol = Chem.MolFromSmiles(generated_smiles)\n",
        "            if mol is not None:\n",
        "                print(f\"Generated molecule (valid SMILES): {generated_smiles}\")\n",
        "                img = MolToImage(mol)\n",
        "                display(img)\n",
        "            else:\n",
        "                print(f\"Generated molecule (invalid SMILES): {generated_smiles}\")\n",
        "\n",
        "            generated_molecules.append(generated_smiles)\n",
        "\n",
        "    return generated_molecules\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Determine the device to use (GPU if available, otherwise CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Define an example protein sequence to predict for\n",
        "    target_protein_sequence = 'MGNASNDSQSEDCETRQWLPPGESPAISSVMFSAGVLGNLIALALLARRWRGDVGCSAGRRSSLSLFHVLVTELVFTDLLGTCLISPVVLASYARNQTLVALAPESRACTYFAFAMTFFSLATMLMLFAMALERYLSIGHPYFYQRRVSRSGGLAVLPVIYAVSLLFCSLPLLDYGQYVQYCPGTWCFIRHGRTAYLQLYATLLLLLIVSVLACNFSVILNLIRMHRRSRRSRCGPSLGSGRGGPGARRRGERVSMAEETDHLILLAIMTITFAVCSLPFTIFAYMNETSSRKEKWDLQALRFLSINSIIDPWVFAILRPPVLRLMRSVLCCRISLRTQDATQTSCSTQSDASKQADL'\n",
        "\n",
        "    # --- Training Process ---\n",
        "    try:\n",
        "        df = pd.read_csv('final_output_15_2_25.csv')\n",
        "        smiles_list = df['SMILES'].tolist()\n",
        "        protein_list = df['TARGET_SEQUENCE'].tolist()\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: The data file 'final_output_15_2_25.csv' was not found. Please ensure it is uploaded.\")\n",
        "        exit()\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    full_dataset = SmilesProteinDataset(smiles_list, protein_list)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Initialize the model and optimizer\n",
        "    protein_embedding_dim = len(get_protein_embedding(target_protein_sequence))\n",
        "    model = ConditionalVAE(\n",
        "        smiles_vocab_size=len(SMILES_TOKEN_DICT),\n",
        "        protein_embedding_dim=protein_embedding_dim,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        latent_dim=LATENT_DIM,\n",
        "        smiles_max_len=SMILES_MAX_LEN,\n",
        "        num_layers=NUM_LAYERS\n",
        "    ).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(MAX_EPOCHS):\n",
        "        print(f\"--- Epoch {epoch+1}/{MAX_EPOCHS} ---\")\n",
        "        train_loss = train_model(model, train_dataloader, optimizer, device)\n",
        "        val_loss = validate_model(model, val_dataloader, device)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{MAX_EPOCHS}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Check for improvement and save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            print(f\"Validation loss improved. Saving model to {MODEL_SAVE_PATH}\")\n",
        "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"Validation loss did not improve. Patience: {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\nTraining complete. Model saved to '{MODEL_SAVE_PATH}'.\")\n",
        "\n",
        "    # --- Generation Process ---\n",
        "    # We load the best model to ensure we are using the one with the best performance\n",
        "    best_model = ConditionalVAE(\n",
        "        smiles_vocab_size=len(SMILES_TOKEN_DICT),\n",
        "        protein_embedding_dim=protein_embedding_dim,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        latent_dim=LATENT_DIM,\n",
        "        smiles_max_len=SMILES_MAX_LEN,\n",
        "        num_layers=NUM_LAYERS\n",
        "    ).to(device)\n",
        "    best_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "    # Generate new molecules using the loaded model\n",
        "    generate_new_molecules(best_model, target_protein_sequence, num_molecules=5, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESstjp_rLM_N"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}